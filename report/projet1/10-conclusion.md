# 5. Conclusion

## 5.1. Résultats Obtenus

L'implémentation d'un clone fonctionnel de `egrep` nous a permis d'explorer différentes approches algorithmiques pour la recherche de motifs. Au-delà de la construction classique d'automates finis (NFA avec $\epsilon$-transitions par construction de Thompson, DFA par méthode des sous-ensembles, minimisation par partitionnement, NFA avec cache DFA à la volée), nous avons intégré des algorithmes de recherche littérale (KMP pour les motifs courts, Boyer-Moore pour les motifs longs, Aho-Corasick pour les alternations de littéraux) et développé des stratégies d'optimisation inspirées de GNU grep, notamment le préfiltrage par extraction de littéraux et la sélection automatique d'algorithme.

Les résultats expérimentaux confirment les prédictions théoriques tout en révélant des comportements intéressants. Le NFA présente une complexité linéaire en pratique pour des textes de taille modérée (< 10KB), tandis que le DFA devient plus efficace pour des textes volumineux une fois son coût de construction amorti. La minimisation du DFA réduit effectivement la taille de la structure, bien que le gain soit souvent marginal par rapport au DFA non minimisé. Le préfiltrage améliore significativement les performances pour les textes volumineux (> 10KB), mais introduit un overhead non négligeable pour les petits textes. Ces observations justifient notre approche de sélection automatique d'algorithme basée sur l'analyse du pattern et de la taille du texte.

## 5.2. Limitations et Points de Discussion

Plusieurs limitations doivent être soulignées. Pour certains patterns pathologiques contenant de nombreuses alternations après une étoile (ex: `(a|b|c|...)*`), le nombre d'états du DFA peut croître exponentiellement ($2^n$ dans le pire cas). Nos tests ont montré un cas avec 4097 états DFA contre 76 états NFA. Dans ces situations, la construction récursive du DFA peut dépasser la limite de la pile d'appels. Une refonte en version itérative serait nécessaire pour garantir la robustesse de l'implémentation.

Par ailleurs, bien que nous ayons mesuré la taille des structures (nombre d'états et de transitions), nous n'avons pas pu mesurer précisément l'empreinte mémoire réelle en production. JavaScript/TypeScript ne fournit pas d'API fiable pour mesurer la consommation mémoire d'objets spécifiques, et les outils de profiling disponibles donnent des résultats approximatifs influencés par le garbage collector et les optimisations du moteur V8. Cette limitation rend difficile l'évaluation précise du coût mémoire des différentes approches, notamment pour comparer le NFA avec cache DFA au DFA complet. Une implémentation en langage bas niveau (C, Rust) permettrait une analyse plus rigoureuse de ces aspects.

D'autres points méritent discussion. Le seuil de 10KB pour activer le préfiltrage a été déterminé empiriquement sur notre corpus de test et pourrait varier selon les caractéristiques du texte et du pattern. L'algorithme de partitionnement utilisé pour la minimisation a une complexité $O(n^2 \times |\Sigma|)$, alors que l'algorithme de Hopcroft, plus efficace en $O(n \log n)$, pourrait améliorer les performances sur de grands automates.

## 5.3. Perspectives

L'extension du support à d'autres opérateurs ERE constitue une évolution naturelle du projet. Les classes de caractères `[a-z]`, les quantificateurs `+` et `?`, ainsi que les ancres `^` et `$` permettraient de couvrir davantage de cas d'usage réels. Cependant, l'ajout de certains opérateurs comme les backreferences `\1` nécessiterait une approche différente. Les backreferences introduisent des dépendances contextuelles qui ne peuvent pas être exprimées par des automates finis réguliers, et requièrent généralement un algorithme de backtracking. Cette approche, bien que plus expressive, présente des risques de complexité exponentielle dans le pire cas, ce qui explique pourquoi de nombreux moteurs modernes limitent leur usage ou les traitent séparément.

Une piste intéressante serait l'exploration des automates finis déterministes tagués (TDFA - Tagged DFA). Cette approche, utilisée notamment par RE2 de Google, permet de capturer les groupes de capture tout en conservant les garanties de complexité linéaire des DFA. Les TDFA associent des tags aux transitions pour marquer les positions de début et de fin des groupes, évitant ainsi le backtracking tout en supportant une partie des fonctionnalités avancées des regex modernes. Cette technique pourrait être particulièrement pertinente pour notre implémentation, car elle s'intègre naturellement dans notre architecture basée sur les automates.

Du côté des optimisations bas niveau, l'utilisation d'instructions SIMD pour la recherche de littéraux pourrait accélérer significativement le préfiltrage, notamment pour les patterns contenant plusieurs littéraux courts. La détection automatique de préfixes fixes dans les patterns permettrait également d'optimiser davantage les cas simples. Pour les fichiers volumineux, la parallélisation du traitement de chunks constitue une piste prometteuse, bien que la gestion des correspondances à cheval sur les frontières de chunks nécessite une attention particulière.

Ce projet a permis de constater que l'efficacité d'un moteur de recherche de motifs repose autant sur le choix algorithmique que sur les optimisations pratiques. La théorie des automates fournit un cadre solide, mais les performances réelles dépendent fortement de l'adaptation des algorithmes aux caractéristiques des données traitées. La sélection automatique d'algorithme et le préfiltrage se révèlent essentiels pour obtenir des performances compétitives sur des cas d'usage variés.
